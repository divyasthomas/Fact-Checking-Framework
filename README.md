# Fact-Checking-Framework
This project is designed to evaluate the accuracy of various fact-checking models using given passages and labeled fact examples. The system supports multiple fact-checking methods and provides detailed evaluation metrics for assessing the performance of these methods.

## Overview

The project is divided into two main files:
1. **`factscore_retrieval_interface.py`**: The main driver script that loads the data, instantiates the selected FactChecker model, executes it on the data, and reports accuracy.
2. **`factcheck.py`**: Contains implementations of various FactChecker models and associated utilities.

## Getting Started

### Prerequisites

Ensure you have the following Python packages installed:
- `torch`
- `numpy`
- `spacy`
- `tqdm`
- `nltk`

You can install the required packages using pip:

```bash
pip install torch numpy spacy tqdm nltk
```

Download the necessary NLTK data:
```
import nltk
nltk.download('stopwords')
nltk.download('punkt')
```

### Running the Code

To run the fact-checking models, You can specify the method to use with the --mode argument when running the script. 
The available modes are:
- random
- always_entail
- word_overlap
- parsing
- entailment

Example:
#### Random Guess Model
```
   python factscore_retrieval_interface.py --mode random
```


## Dataset

The dataset for this project is derived from the FActScore (Min et al., 2023) research. FActScore focuses on evaluating errors in biographies generated by ChatGPT, particularly assessing the accuracy of factual claims extracted from these biographies.

### Overview

FActScore's methodology involves:
1. **Decomposing Claims**: Breaking down generated biographies into individual claims.
2. **Fact Retrieval**: Searching Wikipedia to find articles that may support these claims.
3. **Human Annotation**: Annotating the claims and retrieved passages to assess the performance of fact-checking systems.

For this project, the decomposition and retrieval processes have already been completed. The dataset comprises:

- **Facts**: Natural language propositions extracted from ChatGPT outputs. These are simple sentences representing individual claims.
- **Passages**: Texts retrieved from Wikipedia that are used to validate the facts.

### Fact Labels

Each fact is hand-labeled by humans with one of three labels:
- **"S"**: Supported
- **"NS"**: Not Supported
- **"IR"**: Irrelevant

For simplicity, this project focuses on predicting between "Supported" (S) and "Not Supported" (NS).

### Retrieval Method

Passages are retrieved using the BM25 algorithm, a traditional sparse retrieval model. Although the FActScore paper also explores a dense retriever, the passages retrieved by BM25 are used for this project due to their slightly better performance in this context.

### Data Structure

- **Fact Examples**: Stored in the `FactExample` class, which includes:
  - **Fact**: A string representing the proposition.
  - **Passages**: A list of dictionaries, each containing a title and text of the retrieved passage.
  - **Label**: The annotation of the fact (S, NS, or IR).

The raw fact data and retrieved passages provide a basis for evaluating the performance of different fact-checking models against human-annotated standards.

- **Labels Path**: The path to the JSONL file containing labeled facts. Default: data/dev_labeled_ChatGPT.jsonl.
- **Passages Path**: The path to the JSONL file containing passages retrieved for the facts. Default: data/passages_bm25_ChatGPT_humfacts.jsonl.

## Detailed Description

### factscore_retrieval_interface.py

This script orchestrates the data processing and evaluation. It:
1. Parses command-line arguments to choose the fact-checking method.
2. Reads the passages and labeled fact examples.
3. Instantiates the appropriate FactChecker model based on the selected mode.
4. Runs the model to predict fact support and evaluates its performance using confusion matrices.

### factcheck.py

This file contains the core implementations:
- **FactExample**: Represents a fact, its associated passages, and its label.
- **EntailmentModel**: Wrapper for a pre-trained entailment model.
- **FactChecker**: Base class for all fact-checking models.
- **RandomGuessFactChecker**: Always returns a random prediction.
- **AlwaysEntailedFactChecker**: Always predicts "Supported".
- **WordRecallThresholdFactChecker**: Uses word overlap and TF-IDF for prediction.
- **EntailmentFactChecker**: Uses a textual entailment model to make predictions.


## Fact-Checking Methods

The project supports these main fact-checking methods:

1. **Word Overlap**: Compares the fact to passages based on word overlap metrics.
2. **Textual Entailment**: Uses a pre-trained entailment model to determine if a fact is logically entailed by passages.
   
#### Word Overlap

The WordRecallThresholdFactChecker class computes TF-IDF vectors for facts and passages to determine the similarity. A cosine similarity score is used to classify the fact as "Supported" or "Not Supported" based on a predefined threshold.

#### Textual Entailment

The EntailmentFactChecker class leverages the DeBERTa-v3 model fine-tuned on multiple entailment datasets. It predicts if a fact is entailed by comparing it against sentences in the passages, selecting the highest score for final classification.

## Evaluation

The script prints evaluation metrics, including accuracy and per-class F1 scores, based on a confusion matrix. The evaluation helps to understand the performance of different fact-checking methods.

based on: https://www.cs.utexas.edu/~gdurrett/courses/online-course/a4.pdf
